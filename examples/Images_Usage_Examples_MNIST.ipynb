{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of XAI-Lib Usage for MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intgrad (Tensorflow Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/intgrad_MNIST.jpg' width=\"20%\" height=\"20%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "# Load Dataset\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(MNIST_x_data_train, MNIST_y_data_train), (MNIST_x_data_test, MNIST_y_data_test) = mnist.load_data()\n",
    "#MNIST_x_data_train = np.load('./datasets/mnist_x_train.npy')\n",
    "#MNIST_y_data_train = np.load('./datasets/mnist_y_train.npy')\n",
    "#MNIST_x_data_test = np.load('./datasets/mnist_x_test.npy')\n",
    "#MNIST_y_data_test = np.load('./datasets/mnist_y_test.npy')\n",
    "\n",
    "# Load the model\n",
    "import tensorflow as tf\n",
    "assert tf.__version__[0]=='2', 'please install tensorflow 2.x'\n",
    "MNIST_CNN_tf = tf.keras.models.load_model('./models/keras_cnn_mnist')\n",
    "MNIST_CNN_tf.trainable=False\n",
    "MNIST_CNN_tf.summary()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "from xailib.explainers.intgrad_explainer import IntgradImageExplainer\n",
    "\n",
    "# 1) Create the Explainer\n",
    "ig = IntgradImageExplainer(MNIST_CNN_tf)\n",
    "\n",
    "# 2) Fit the Explainer\n",
    "ig.fit()\n",
    "\n",
    "# 3) Select the image we want to give an explanation for\n",
    "image = MNIST_x_data_train[0,:]\n",
    "\n",
    "# 4a) Preprocessing function: function that takes as input an image and return the correct format for the black box\n",
    "def preprocessing(image):\n",
    "    return tf.convert_to_tensor(image.reshape(28,28,1),dtype=tf.float32)\n",
    "\n",
    "# 4b) Predict Function of our black box: It needs to take as input the output of preprocessing and return an array of probabilities of classes with shape (-1,num_classes)\n",
    "def predict(image):\n",
    "    return tf.nn.softmax(MNIST_CNN_tf(image), axis=-1)\n",
    "\n",
    "# 5) Explain an Instance\n",
    "tf_scores = ig.explain(image, 5, 'black', preprocessing, predict, model_type='tensorflow')\n",
    "\n",
    "plt.imshow(tf_scores,cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.axis('off');"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intgrad (PyTorch Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "# Load Dataset\n",
    "#import torchvision\n",
    "#import torchvision.datasets as datasets\n",
    "#mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "#mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "#from tensorflow.keras.datasets import mnist\n",
    "#(MNIST_x_data_train, MNIST_y_data_train), (MNIST_x_data_test, MNIST_y_data_test) = mnist.load_data()\n",
    "\n",
    "# Load the model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "        )\n",
    "        \n",
    "        self.linear_block = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(128*7*7, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.nn.functional.softmax(self.linear_block(x),dim=1)\n",
    "        \n",
    "        return x\n",
    "MNIST_CNN = Net()\n",
    "MNIST_CNN.load_state_dict(torch.load('./models/MNIST_Pytorch_CNN.pt',map_location=torch.device('cpu')))\n",
    "MNIST_CNN.eval()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "from xailib.explainers.intgrad_explainer import IntgradImageExplainer\n",
    "from skimage.color import gray2rgb\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[..., :3], [0.299, 0.587, 0.114])\n",
    "\n",
    "# 1) Create the Explainer\n",
    "ig = IntgradImageExplainer(MNIST_CNN)\n",
    "\n",
    "# 2) Fit the Explainer\n",
    "ig.fit()\n",
    "\n",
    "# 3) Select the image we want to give an explanation for\n",
    "image = MNIST_x_data_train[0]\n",
    "\n",
    "# 4a) Preprocessing function: function that takes as input an image and return the correct format for the black box\n",
    "def preprocessing(image):\n",
    "    return torch.tensor([[image]]).float()\n",
    "\n",
    "# 4b) Predict Function of our black box: It needs to take as input the output of preprocessing and return an array of probabilities of classes with shape (-1,num_classes)\n",
    "def predict(image):\n",
    "    return MNIST_CNN(image)\n",
    "\n",
    "# 5) Explain an Instance\n",
    "scores = ig.explain(image, 5, 'black', preprocessing, predict, model_type='pytorch')\n",
    "\n",
    "plt.imshow(scores,cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.axis('off');"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/lime_MNIST.jpg' width=\"50%\" height=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "from xailib.explainers.lime_explainer import LimeXAIImageExplainer\n",
    "from skimage.color import label2rgb, gray2rgb, rgb2gray\n",
    "\n",
    "# index of the image to explain\n",
    "idx = 0\n",
    "\n",
    "# 1) Create the Explainer\n",
    "lm = LimeXAIImageExplainer(MNIST_CNN)\n",
    "\n",
    "# 2a) Fit the Explainer\n",
    "lm.fit()\n",
    "\n",
    "# 2b) Create custom classifier function if not predict \n",
    "# function that takes as input an array of images (the LIME neighbourhood) and return an array of (num_images,num_classes)\n",
    "# If None will use black_box.predict function\n",
    "def classifier_fn(images):\n",
    "    images = torch.tensor(rgb2gray(images).reshape(-1,1,28,28)).float()\n",
    "    return np.argmax(MNIST_CNN(images).detach().numpy(),axis=1).reshape(-1,1)\n",
    "\n",
    "# 3) Explain an Instance\n",
    "explanation = lm.explain(gray2rgb(MNIST_x_data_train[idx]), classifier_fn, num_samples=500)\n",
    "\n",
    "# 4) Plot the results\n",
    "lm.plot_lime_values(MNIST_x_data_train[idx], explanation)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "from skimage.segmentation import mark_boundaries\n",
    "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=False)\n",
    "img_boundry1 = mark_boundaries(temp/255.0, mask)\n",
    "plt.imshow(img_boundry1)\n",
    "plt.axis('off');"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=False, num_features=10, hide_rest=False)\n",
    "img_boundry2 = mark_boundaries(temp/255.0, mask)\n",
    "plt.imshow(img_boundry2);"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "from skimage.segmentation import quickshift, mark_boundaries\n",
    "from skimage.color import gray2rgb\n",
    "image = gray2rgb(MNIST_x_data_train[0].astype(int))\n",
    "\"\"\"\n",
    "First you need to define the segmentation function to use, in this example we will use\n",
    "quickshift from skimage. It is foundamental to tune the parameters of the segmentation in\n",
    "order to have a good one. It is recommentded to print an example of just only the segmentation\n",
    "to see if it works for that kind of image\n",
    "For quickshift we have to tune these two parameters below\n",
    "kernel_size : float, optional\n",
    "    Width of Gaussian kernel used in smoothing the\n",
    "    sample density. Higher means fewer clusters.\n",
    "max_dist : float, optional\n",
    "    Cut-off point for data distances.\n",
    "    Higher means fewer clusters.\n",
    "\"\"\"\n",
    "def segmentation_fn(image):\n",
    "    return quickshift(image, kernel_size=2, max_dist=3)\n",
    "plt.imshow(mark_boundaries(image/255,segmentation_fn(image)))\n",
    "plt.axis('off');"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "from xailib.explainers.lime_explainer import LimeXAIImageExplainer\n",
    "from skimage.color import label2rgb, gray2rgb, rgb2gray\n",
    "\n",
    "# index of the image to explain\n",
    "idx = 0\n",
    "\n",
    "# 1) Create the Explainer\n",
    "lm = LimeXAIImageExplainer(MNIST_CNN)\n",
    "\n",
    "# 2a) Fit the Explainer\n",
    "lm.fit()\n",
    "\n",
    "# 2b) Create custom classifier function if not predict \n",
    "# function that takes as input an array of images (the LIME neighbourhood) and return an array of (num_images,num_classes)\n",
    "# If None will use black_box.predict function\n",
    "def classifier_fn(images):\n",
    "    images = torch.tensor(rgb2gray(images).reshape(-1,1,28,28)).float()\n",
    "    return np.argmax(MNIST_CNN(images).detach().numpy(),axis=1).reshape(-1,1)\n",
    "\n",
    "# 3) Explain an Instance\n",
    "explanation = lm.explain(gray2rgb(MNIST_x_data_train[idx]), classifier_fn, segmentation_fn, num_samples=500)\n",
    "\n",
    "# 4) Plot the results\n",
    "lm.plot_lime_values(MNIST_x_data_train[idx], explanation)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RISE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/rise_MNIST.jpg' width=\"50%\" height=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "from xailib.explainers.rise_explainer import RiseXAIImageExplainer\n",
    "\n",
    "class Rise_model_wrapper():\n",
    "    def __init__(self, bb, input_size):\n",
    "        self.model = bb\n",
    "        self.input_size = input_size\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.model(torch.tensor(X).reshape(-1,1,28,28).float()).detach().numpy()\n",
    "        \n",
    "\n",
    "model = Rise_model_wrapper(MNIST_CNN, (28,28))\n",
    "rise = RiseXAIImageExplainer(model)\n",
    "        \n",
    "\n",
    "N = 10 # number of random masks\n",
    "s = 10 # cell_size = input_shape / s\n",
    "p1 = 0.5 # masking probability\n",
    "\n",
    "rise.fit(N, s, p1)\n",
    "\n",
    "F,ax=plt.subplots(1,10,figsize=(30,5))\n",
    "for i in range(10):\n",
    "    ax[i].imshow(rise.masks[i,:])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(1,3,figsize=(10,5))\n",
    "img = MNIST_x_data_train[0,:]\n",
    "N = 4000\n",
    "s = 8\n",
    "p1 = 0.5\n",
    "\n",
    "rise.fit(N, s, p1)\n",
    "\n",
    "sal = rise.explain(img.reshape(28,28,1)/255)\n",
    "\n",
    "ax[0].imshow(img,cmap='gray')\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(sal[MNIST_y_data_test[0]],cmap='jet')\n",
    "ax[1].axis('off')\n",
    "\n",
    "ax[2].imshow(img,cmap='gray')\n",
    "ax[2].imshow(sal[MNIST_y_data_test[0]],cmap='jet',alpha=0.5)\n",
    "ax[2].axis('off');"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRADCAM\n",
    "Weight the 2D activations by the average gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "from xailib.explainers.gradcam_explainer import GradCAMImageExplainer\n",
    "\n",
    "img = MNIST_x_data_train[0,:]\n",
    "\n",
    "explainer = GradCAMImageExplainer(MNIST_CNN)\n",
    "\n",
    "explainer.fit([MNIST_CNN.conv_block[10]])\n",
    "\n",
    "attr = explainer.explain(torch.tensor(img).reshape(-1,1,28,28).float()/255, 5, model_type='pytorch')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(1,3,figsize=(10,5))\n",
    "\n",
    "ax[0].imshow(img,cmap='gray')\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(attr[0,:],cmap='jet')\n",
    "ax[1].axis('off')\n",
    "\n",
    "ax[2].imshow(img,cmap='gray')\n",
    "ax[2].imshow(attr[0,:],cmap='jet',alpha=0.5)\n",
    "ax[2].axis('off');"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "from xailib.explainers.gradcam_explainer import GradCAMImageExplainer\n",
    "\n",
    "img = MNIST_x_data_train[0,:]\n",
    "\n",
    "explainer = GradCAMImageExplainer(MNIST_CNN_tf)\n",
    "\n",
    "explainer.fit('max_pooling2d_1', model_type='tensorflow')\n",
    "\n",
    "attr = explainer.explain(tf.convert_to_tensor(img.reshape(28,28,1)/255,dtype=tf.float32), 5, model_type='tensorflow')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(1,3,figsize=(10,5))\n",
    "\n",
    "ax[0].imshow(img,cmap='gray')\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(attr,cmap='jet')\n",
    "ax[1].axis('off')\n",
    "\n",
    "ax[2].imshow(img,cmap='gray')\n",
    "ax[2].imshow(attr,cmap='jet',alpha=0.5)\n",
    "ax[2].axis('off');"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRADCAM++\n",
    "Like GradCAM but uses second order gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "from xailib.explainers.gradcam_explainer import GradCAMPlusPlusImageExplainer\n",
    "\n",
    "img = MNIST_x_data_train[0,:]\n",
    "\n",
    "explainer = GradCAMPlusPlusImageExplainer(MNIST_CNN)\n",
    "\n",
    "explainer.fit([MNIST_CNN.conv_block[10]])\n",
    "\n",
    "attr = explainer.explain(torch.tensor(img).reshape(-1,1,28,28).float()/255, 5, model_type='pytorch')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(1,3,figsize=(10,5))\n",
    "\n",
    "ax[0].imshow(img,cmap='gray')\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(attr[0,:],cmap='jet')\n",
    "ax[1].axis('off')\n",
    "\n",
    "ax[2].imshow(img,cmap='gray')\n",
    "ax[2].imshow(attr[0,:],cmap='jet',alpha=0.5)\n",
    "ax[2].axis('off');"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "source": [
    "from xailib.explainers.gradcam_explainer import GradCAMPlusPlusImageExplainer\n",
    "\n",
    "img = MNIST_x_data_train[0,:]\n",
    "\n",
    "explainer = GradCAMPlusPlusImageExplainer(MNIST_CNN_tf)\n",
    "\n",
    "explainer.fit('max_pooling2d_1', model_type='tensorflow')\n",
    "\n",
    "attr = explainer.explain(tf.convert_to_tensor(img.reshape(28,28,1)/255,dtype=tf.float32), 5, model_type='tensorflow')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(1,3,figsize=(10,5))\n",
    "\n",
    "ax[0].imshow(img,cmap='gray')\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(attr,cmap='jet')\n",
    "ax[1].axis('off')\n",
    "\n",
    "ax[2].imshow(img,cmap='gray')\n",
    "ax[2].imshow(attr,cmap='jet',alpha=0.5)\n",
    "ax[2].axis('off');"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ILORE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/lore_MNIST.jpg' width=\"50%\" height=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "source": [
    "from skimage.segmentation import quickshift, mark_boundaries\n",
    "from skimage.color import gray2rgb\n",
    "image = gray2rgb(MNIST_x_data_train[0].astype(int))\n",
    "\"\"\"\n",
    "First you need to define the segmentation function to use, in this example we will use\n",
    "quickshift from skimage. It is foundamental to tune the parameters of the segmentation in\n",
    "order to have a good one. It is recommentded to print an example of just only the segmentation\n",
    "to see if it works for that kind of image\n",
    "For quickshift we have to tune these two parameters below\n",
    "kernel_size : float, optional\n",
    "    Width of Gaussian kernel used in smoothing the\n",
    "    sample density. Higher means fewer clusters.\n",
    "max_dist : float, optional\n",
    "    Cut-off point for data distances.\n",
    "    Higher means fewer clusters.\n",
    "\"\"\"\n",
    "def segmentation_fn(image):\n",
    "    return quickshift(image, kernel_size=2, max_dist=3)\n",
    "plt.imshow(mark_boundaries(image/255,segmentation_fn(image)))\n",
    "plt.axis('off');"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "source": [
    "from externals.LOREM.ilorem import ILOREM\n",
    "\n",
    "# Function to convert from rgb2gray since the one implemented in skimage gives problems\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[..., :3], [0.299, 0.587, 0.114])\n",
    "\n",
    "# Predict Function of our black box. \n",
    "# !!!IMPORTANT!!! It needs to take as input a list of RGB images and return an array of class indices of shape (-1,1)\n",
    "def classifier_fn(images):\n",
    "    images = torch.tensor(rgb2gray(np.array(images))/255).float().unsqueeze(1)\n",
    "    return np.argmax(MNIST_CNN(images).detach().numpy(),axis=1).reshape(-1,1)\n",
    "\n",
    "# Create the explainer\n",
    "explainer = ILOREM(bb_predict = classifier_fn, \n",
    "                   class_name='class', \n",
    "                   class_values = range(10), \n",
    "                   segmentation_fn=segmentation_fn,\n",
    "                   verbose=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "source": [
    "exp = explainer.explain_instance(gray2rgb(MNIST_x_data_train[0].astype(int)), num_samples=100, use_weights=True, metric='cosine')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "source": [
    "print('e = {\\n\\tr = %s\\n\\tc = %s    \\n}' % (exp.rstr(), exp.cstr()))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "source": [
    "x,y = np.meshgrid(range(28),range(28))\n",
    "img2show, mask = exp.get_image_rule(hide_rest=False, num_features=None, min_importance=0.0)\n",
    "plt.imshow(mark_boundaries(img2show/255,mask))\n",
    "mask[mask != 2] = -1\n",
    "mask[mask == 2] = segmentation_fn(image)[mask == 2]\n",
    "\n",
    "mask = mask.ravel()\n",
    "\n",
    "for i in np.unique(mask):\n",
    "    if i != -1:\n",
    "        x_c = x.ravel()[np.where(mask==i)[0]]\n",
    "        y_c = y.ravel()[np.where(mask==i)[0]]\n",
    "        plt.text(np.median(x_c),np.median(y_c),str(i),fontsize=15, weight='bold')\n",
    "plt.axis('off');"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "source": [
    "imgs2show, masks, coutcomes = exp.get_image_counterfactuals(hide_rest=False, num_features=None, min_importance=0.0)\n",
    "x,y = np.meshgrid(range(28),range(28))\n",
    "s = segmentation_fn(image)\n",
    "\n",
    "if len(imgs2show)==1:\n",
    "    plt.imshow(mark_boundaries(imgs2show[0]/255, masks[0]))\n",
    "    \n",
    "    mask = masks[0]\n",
    "    mask[mask != 1] = -1\n",
    "    mask[mask == 1] = s[mask == 1]\n",
    "    mask = mask.ravel()\n",
    "    for j in np.unique(mask):\n",
    "        if j != -1:\n",
    "            x_c = x.ravel()[np.where(mask==j)[0]]\n",
    "            y_c = y.ravel()[np.where(mask==j)[0]]\n",
    "            plt.text(np.median(x_c),np.median(y_c),str(j),fontsize=15, weight='bold')\n",
    "            \n",
    "    plt.title('Black Box Prediction: '+str(coutcomes[0]))\n",
    "    plt.axis('off')\n",
    "else:\n",
    "    F, ax = plt.subplots(1,len(imgs2show),figsize=(5*len(imgs2show),5))\n",
    "    for i in range(len(imgs2show)):\n",
    "        ax[i].imshow(mark_boundaries(imgs2show[i]/255, masks[i]))\n",
    "        \n",
    "        mask = masks[i]\n",
    "        mask[mask != 1] = -1\n",
    "        mask[mask == 1] = s[mask == 1]\n",
    "        mask = mask.ravel()\n",
    "        for j in np.unique(mask):\n",
    "            if j != -1:\n",
    "                x_c = x.ravel()[np.where(mask==j)[0]]\n",
    "                y_c = y.ravel()[np.where(mask==j)[0]]\n",
    "                ax[i].text(np.median(x_c),np.median(y_c),str(j),fontsize=15, weight='bold')\n",
    "        \n",
    "        ax[i].set_title('Black Box Prediction: '+str(coutcomes[i]))\n",
    "        ax[i].axis('off')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABELE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to work ABELE needs a Test Set and an autoencoder trained on that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Autoencoder Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a script to guide the user to the creation and the training of the autoencoder. \n",
    "If you want to use your own, you need to implement an autoencoder class with these funtioncs inside:\n",
    "- self.encode: take the image as input and return the latent space vector\n",
    "- self.decode: take a vector as input and return the reconstructed image\n",
    "- self.latent_dim: int representing the number of dimensions of the latent space\n",
    "- self.discriminator: (only for adversarial autoencoders) take a vector as input and output a class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "source": [
    "import sys\n",
    "import os\n",
    "import skimage\n",
    "from externals.ABELE.experiments.exputil import get_autoencoder\n",
    "from externals.ABELE.experiments.exputil import get_dataset\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#disable eager execution in tensorflow 2.x for faster training time\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()\n",
    "\n",
    "# The dataset to use for training\n",
    "# if you prefer to use yours load the data in two vector X_data and Y_data, set dataset variable to a custom name and custom_dataset to True\n",
    "dataset = 'mnist'\n",
    "custom_dataset = False\n",
    "if not custom_dataset:\n",
    "    _, _, X_test, Y_test, use_rgb = get_dataset(dataset)\n",
    "\n",
    "# The type of autoencoder to train ['aae','vae']\n",
    "ae_name = 'aae'\n",
    "\n",
    "# This script will save the models weights in the folder ./path/models/abele/dataset/ae_name\n",
    "path = './'\n",
    "path_aemodels = path + 'models/abele/%s/%s/' % (dataset,ae_name)\n",
    "if not os.path.exists(path_aemodels):\n",
    "    os.makedirs(path_aemodels)\n",
    "\n",
    "# HYPER-PARAMETERS\n",
    "#define the epochs and the batch size for which the autoencoder will be trained\n",
    "epochs = 2000\n",
    "batch_size = 64\n",
    "#Print training summary every sample_interval step\n",
    "sample_interval = 50\n",
    "\n",
    "\"\"\"\n",
    " Get_autoencoder create the autoencoder class to train\n",
    " Arguments: \n",
    "    X: dataset to use to train the ae \n",
    "    ae_name: type of the autoencoder to train, only adversarial autoencoder (aae) and variational autoencoder (vae) are supported\n",
    "    path_aemodels: path where to save the weights\n",
    "    dataset: name of the dataset to use\n",
    "    OPTIONAL:\n",
    "    latent_dim: latent space dimension (only use if dataset is custom)\n",
    "    hidden_dim: hidden dimension of the autoencoders layers (ex: 1024 will create two layers of 1024 hidden units) (only use if dataset is custom)\n",
    "    num_filters: number of filter to use in the convolutional layers (only use if dataset is custom)\n",
    "\"\"\"\n",
    "ae = get_autoencoder(X_test, ae_name, path_aemodels, dataset)\n",
    "\n",
    "\"\"\" Fit method\n",
    " Arguments: \n",
    "    X: Dataset to use to train the ae\n",
    "    epochs: Epochs to train\n",
    "    batch_size\n",
    "    sample interval (see before)\n",
    "\"\"\"\n",
    "ae.fit(X_test, epochs=epochs, batch_size=batch_size, sample_interval=sample_interval)\n",
    "# save the weights in the oath specified before\n",
    "ae.save_model()\n",
    "# save also some sample images to test if the autoencoderis trained correctly\n",
    "ae.sample_images(epochs)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "source": [
    "ae.load_model()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from skimage.color import gray2rgb, rgb2gray\n",
    "from skimage import feature, transform\n",
    "\n",
    "from xailib.explainers.abele_explainer import ABELEImageExplainer\n",
    "from externals.ABELE.experiments.exputil import get_dataset\n",
    "from externals.ABELE.experiments.exputil import get_autoencoder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "random_state = 0\n",
    "dataset = 'mnist' \n",
    "black_box = 'AB' #agnostic Black Box\n",
    "\n",
    "# load autoencoder and black box\n",
    "ae_name = 'aae' \n",
    "path = './' \n",
    "path_aemodels = path + 'models/abele/%s/%s/' % (dataset, ae_name)\n",
    "bb = tf.keras.models.load_model(\"./models/cnn_simple_mnist_no_pickle\")\n",
    "\n",
    "# defining a functions for bb to return the class index value\n",
    "def bb_predict(X):\n",
    "    X = X.astype(float)\n",
    "    Y = bb.predict(rgb2gray(X).reshape(-1,28,28,1))   \n",
    "    return np.argmax(Y, axis=1)\n",
    "\n",
    "# load data\n",
    "_, _, X_test, Y_test, _ = get_dataset(dataset)\n",
    "\n",
    "# load auto encoder\n",
    "ae = get_autoencoder(X_test, ae_name, path_aemodels, dataset)\n",
    "ae.load_model()\n",
    "\n",
    "# index Image 2 Explain\n",
    "i2e = 24\n",
    "img = X_test[i2e]\n",
    "# time\n",
    "start = time.time()\n",
    "\n",
    "# create explainer\n",
    "\"\"\"\n",
    "Arguments:\n",
    "    bb_predict: function which return the prediction of the blackbox in form index of the class \n",
    "    class_name: name of the class used when printing rules (class_name: class_value)\n",
    "    class_values: list of names of the classes (class_name: class_value)\n",
    "    neigh_type: select the nighbourhood type,\n",
    "                supportecd types:\n",
    "                'gnt' : genetic\n",
    "                'rnd' : random\n",
    "                'hrg' : hybrid-random-genetic\n",
    "                'gntp': genetic probabilistic\n",
    "                'hrgp': hybrid probabilistic\n",
    "    ocr: [0.1] other class values, ratio of other class from the one predicted in the neighbourhood\n",
    "    kernel: [None] Kernel to weights the point in the nieghbourhood\n",
    "    kernel_width : [None]  \n",
    "    autoencoder: Autoencoder to generate the latent space points\n",
    "    use_rgb = [True] Set to True if the input images are rgb, False for grayscale\n",
    "    filter_crules: [None] if True Prototypes are checked by the black box to be the same class of the query image\n",
    "    random_state: set the seed of the random state\n",
    "    verbose: True if you want to print more informations\n",
    "    NEIGHBOURHOOD PARAMETERS: the following parameters are Neighbourhood specific and may not apply to all of the neighbourhood types\n",
    "        valid_thr: [0.5] threshold to change class in the autoencoder disciminator\n",
    "        alpha1: [0.5] weight of the feature similarity of the neighbourhood points\n",
    "        alpha2: [0.5] weight of the target similarity of the neighbourhood points\n",
    "        ngen: [100] number of generations of the genetic algorithm\n",
    "        mutpb: [0.2] The probability of mutating an individual in the genetic algorithm\n",
    "        cxpb: [0.5] The probability of mating two individuals in the genetic algorithm\n",
    "        tournsize: [3] number of tournaments in the genetic algorithm\n",
    "        hallooffame_ratio: [0.1] Fraction of exemplars to keep at every genetic generation\n",
    "\"\"\"\n",
    "config = {'bb_predict':bb_predict,\n",
    "          'class_name':'class',\n",
    "          'class_values':['%s' % i for i in range(len(np.unique(Y_test)))],\n",
    "          'neigh_type':'hrg', \n",
    "          'ocr':0.1, #other class ratio in the neighbourhood\n",
    "          'kernel_width':None, \n",
    "          'kernel':None, \n",
    "          'autoencoder':ae, \n",
    "          'use_rgb':True, \n",
    "          'filter_crules':True, \n",
    "          'random_state':random_state, \n",
    "          'verbose':True, \n",
    "          'valid_thr':0.5,\n",
    "          'alpha1':0.5, \n",
    "          'alpha2':0.5, \n",
    "          'ngen':100, \n",
    "          'mutpb':0.2, \n",
    "          'cxpb':0.5, \n",
    "          'tournsize':3, \n",
    "          'halloffame_ratio':0.1}\n",
    "\n",
    "\n",
    "explainer = ABELEImageExplainer(bb)\n",
    "explainer.fit(config)\n",
    "\n",
    "#generate explanation\n",
    "\"\"\"\n",
    "generate an explanation for a given image\n",
    "Arguments:\n",
    "    img: the image to explain\n",
    "    num_samples: [1000] number of samples to generate with the neighbourhood algorithm\n",
    "    use_weights: [True] if weights the points using distance\n",
    "Return:\n",
    "Explanation object compose by several things\n",
    "    rstr: string describing the rule\n",
    "    cstr: string describing the counterfactual rule\n",
    "    bb_pred: black box prediction of the image\n",
    "    dt_pred: decisoon tree prediction\n",
    "    fidelity: fidelity between black box and the decision tree\n",
    "    limg: latent space representation of the image\n",
    "\"\"\"\n",
    "exp = explainer.explain(img, num_samples=300, use_weights=True)\n",
    "\n",
    "# time\n",
    "end = time.time()\n",
    "print('--------------------------')\n",
    "print('execution time: ',end - start,' sec')\n",
    "print('e = {\\n\\tr = %s\\n\\tc = %s    \\n}' % (exp.getRules(), exp.getCounterfactualRules()))\n",
    "print('--------------------------')\n",
    "print('bb prediction of the image: ',exp.exp.bb_pred,'dt prediction: ',exp.exp.dt_pred,'fidelity: ',exp.exp.fidelity)\n",
    "print('latent space representation: ',exp.exp.limg)\n",
    "\n",
    "\"\"\"\n",
    "Arguments:\n",
    "    features: [None] list of which feature of the latent space to use, If None use all\n",
    "    samples: [10] number of prototype to use\n",
    "Return the image and the difference between the prototypes\n",
    "\"\"\"\n",
    "img2show, mask = exp.getFeaturesImportance(features=None, samples=400)\n",
    "\n",
    "# Plot Script\n",
    "F, ax = plt.subplots(1,2, figsize=(10,5))\n",
    "if use_rgb:\n",
    "    ax[0].imshow(img2show)\n",
    "else:\n",
    "    ax[0].imshow(img2show, cmap='gray')\n",
    "bbo = bb_predict(np.array([img2show]))[0]\n",
    "ax[0].set_title('Image to explain - black box %s' % bbo)\n",
    "ax[0].axis('off')\n",
    "dx, dy = 0.05, 0.05\n",
    "xx = np.arange(0.0, img2show.shape[1], dx)\n",
    "yy = np.arange(0.0, img2show.shape[0], dy)\n",
    "xmin, xmax, ymin, ymax = np.amin(xx), np.amax(xx), np.amin(yy), np.amax(yy)\n",
    "extent = xmin, xmax, ymin, ymax\n",
    "cmap_xi = plt.get_cmap('Greys_r')\n",
    "cmap_xi.set_bad(alpha=0)\n",
    "# Compute edges (to overlay to heatmaps later)\n",
    "percentile = 100\n",
    "dilation = 3.0\n",
    "alpha = 0.8\n",
    "xi_greyscale = img2show if len(img2show.shape) == 2 else np.mean(img2show, axis=-1)\n",
    "in_image_upscaled = transform.rescale(xi_greyscale, dilation, mode='constant')\n",
    "edges = feature.canny(in_image_upscaled).astype(float)\n",
    "edges[edges < 0.5] = np.nan\n",
    "edges[:5, :] = np.nan\n",
    "edges[-5:, :] = np.nan\n",
    "edges[:, :5] = np.nan\n",
    "edges[:, -5:] = np.nan\n",
    "overlay = edges\n",
    "ax[1].imshow(mask, extent=extent, cmap=plt.cm.BrBG, alpha=1, vmin=0, vmax=255)\n",
    "ax[1].imshow(overlay, extent=extent, interpolation='none', cmap=cmap_xi, alpha=alpha)\n",
    "ax[1].axis('off')\n",
    "ax[1].set_title('Attention area respecting latent rule');"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Green Points are the ones critical for the prediction, also called exemplars\n",
    "\n",
    "Yellow points are the ones who change class if activated\n",
    "\n",
    "White Points are do-nothing points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Return the prototypes images\n",
    "Arguments:\n",
    "    num_prototypes: [5] number of prototypes to return\n",
    "    return_latent: [False] if True return latent representation\n",
    "    return_diff: [False] If True return the difference with the query image\n",
    "    features: [None] list of the features int he latent space to use, if none use all\n",
    "\"\"\"\n",
    "proto = exp.getExemplars(num_prototypes=5)\n",
    "\n",
    "F, ax = plt.subplots(1,5,figsize=(30,5))\n",
    "for i in range(5):\n",
    "    ax[i].imshow(proto[i]/255)\n",
    "    ax[i].axis('off')\n",
    "    ax[i].set_title('model prediction: '+str(bb_predict(proto[i])[0]))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Return the couterfactuals satisfying the counterfactual rule\n",
    "\"\"\"\n",
    "counter = exp.getCounterExemplars()\n",
    "\n",
    "F, ax = plt.subplots(1,len(counter),figsize=(30,5))\n",
    "for i in range(len(counter)):\n",
    "    ax[i].imshow(counter[i]/255)\n",
    "    ax[i].axis('off')\n",
    "    ax[i].set_title('model prediction: '+str(bb_predict(counter[i])[0]))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "source": [
    "image = MNIST_x_data_train[0:1]\n",
    "plt.imshow(tf_scores,cmap='coolwarm')\n",
    "plt.axis(False);"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "source": [
    "from xailib.metrics.insertiondeletion import ImageInsDel\n",
    "import torch\n",
    "\n",
    "# Load the model\n",
    "import tensorflow as tf\n",
    "MNIST_CNN_tf = tf.keras.models.load_model('./models/keras_cnn_mnist')\n",
    "MNIST_CNN_tf.trainable=False\n",
    "\n",
    "def predict(image):\n",
    "    return MNIST_CNN_tf.predict(image.reshape(1,28,28,1)/255) \n",
    "\n",
    "step = 28\n",
    "mode = 'del'\n",
    "metric = ImageInsDel(predict, mode, step, torch.zeros_like)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "source": [
    "from sklearn.metrics import auc\n",
    "\n",
    "y = metric(image, 28, tf_scores, rgb=False)\n",
    "x = np.arange(len(y))*step/784\n",
    "x[-1] = 1.0\n",
    "print('Score:', auc(x, y))\n",
    "\n",
    "plt.plot(x, y, label=np.round(auc(x, y),4))\n",
    "plt.fill_between(x, y, alpha=0.4)\n",
    "plt.xlabel('Percentage of pixel removed')\n",
    "plt.ylabel('Accuracy of the model')\n",
    "plt.legend();"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "source": [
    "from scipy.ndimage.filters import gaussian_filter\n",
    "CH = 1\n",
    "def gkern(klen, nsig):\n",
    "    \"\"\"Returns a Gaussian kernel array.\n",
    "    Convolution with it results in image blurring.\"\"\"\n",
    "    # create nxn zeros\n",
    "    inp = np.zeros((klen, klen))\n",
    "    # set element at the middle to one, a dirac delta\n",
    "    inp[klen//2, klen//2] = 1\n",
    "    # gaussian-smooth the dirac, resulting in a gaussian filter mask\n",
    "    k = gaussian_filter(inp, nsig)\n",
    "    kern = np.zeros((CH, CH, klen, klen))\n",
    "    for i in range(CH):\n",
    "        kern[i, i] = k\n",
    "    return torch.from_numpy(kern.astype('float32'))\n",
    "\n",
    "# Function that blurs input image\n",
    "def blur(image, klen=11, ksig=5):\n",
    "\n",
    "    kern = gkern(klen, ksig)\n",
    "    image = torch.tensor(np.expand_dims(image, 0)).float()\n",
    "    return nn.functional.conv2d(image, kern, padding=klen//2)[0,:]\n",
    "\n",
    "F,ax = plt.subplots(1,2)\n",
    "ax[0].imshow(image[0,:], cmap='gray')\n",
    "ax[0].axis(False)\n",
    "ax[1].imshow(blur(image, 11, 5)[0,:],cmap='gray')\n",
    "ax[1].axis(False);"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "source": [
    "from xailib.metrics.insertiondeletion import ImageInsDel\n",
    "\n",
    "def predict(image):\n",
    "    return MNIST_CNN_tf.predict(image.reshape(1,28,28,1)/255) \n",
    "\n",
    "step = 10\n",
    "mode = 'ins'\n",
    "metric = ImageInsDel(predict, mode, step, blur)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "source": [
    "from sklearn.metrics import auc\n",
    "\n",
    "y = metric(image, 28, scores, rgb=False)\n",
    "x = np.arange(len(y))*step/784\n",
    "x[-1] = 1.0\n",
    "print('Score:', auc(x, y))\n",
    "\n",
    "plt.plot(x, y, label=np.round(auc(x, y),4))\n",
    "plt.fill_between(x, y, alpha=0.4)\n",
    "plt.xlabel('Percentage of pixel removed')\n",
    "plt.ylabel('Accuracy of the model')\n",
    "plt.legend();"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "source": [
    "y = metric(image, 28, scores, rgb=False, verbose=2)\n",
    "print('Score:', auc(x, y))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
